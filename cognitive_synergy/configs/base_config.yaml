# cognitive_synergy/configs/base_config.yaml
# Base configuration for the Cognitive Synergy Model MVRP v1
# --- MODIFIED FOR FREE COLAB (T4 GPU TARGET, ~15GB VRAM) ---

# === Project Settings ===
project_name: "cognitive_synergy_mvrp_v1"
seed: 42 # For reproducibility across runs
device: "cuda" # Ensure Colab Runtime is set to GPU!

# === Backbone Configuration ===
backbones:
  vision:
    model_name: "facebook/dinov2-base" # Reasonable size (~86M params) for T4
    pretrained: true
  language:
    # CRITICAL CHANGE: Switched to a much smaller model (~110M params)
    # 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B' (1.5B params) is too large for free Colab VRAM.
    model_name: "bert-base-uncased"
    # model_name: "microsoft/deberta-v3-base" # Another good alternative (~180M params)
    pretrained: true

# === Interface Layers ===
# Indices of layers to extract features from for interfacing
interface_layers:
  # Indices need to be valid for the chosen models (bert-base has 12 layers, 0-11)
  language: [3, 7, 11] # Example for 12-layer BERT
  vision: [2, 6, 10]   # Example for 12-layer DINOv2-Base

# === Interface Module Configuration ===
# Settings for each BiDirectionalInterfaceModule instance (kept defaults)
interface_module:
  cross_attention_hidden_dim: 768
  n_heads: 8
  use_film: false
  pooling_type: "attention"
  attention_pooling_heads: 1
  workspace_output_dim: 256

# === Shared Workspace Configuration ===
# Settings for the module that fuses interface contributions (kept defaults)
shared_workspace:
  num_layers: 4
  hidden_dim: 768
  output_dim: 512

# === Contrastive Projection Head ===
# Projects world_state and CLS tokens to a common space for contrastive loss (kept defaults)
contrastive_head:
  projection_dim: 256
  use_projection: true

# === Loss Configuration ===
loss:
  type: "contrastive_alignment"
  temperature: 0.07
  symmetric: true

# === Training Parameters ===
training:
  epochs: 20 # Keep desired epochs, but be mindful of Colab time limits
  # MODIFIED: Increased batch_size significantly due to smaller LLM.
  # Adjust lower if you get Out-Of-Memory (OOM) errors. Start maybe with 8 or 16.
  batch_size: 16 # Per-device batch size
  # MODIFIED: Reduced workers based on Colab warnings & faster local I/O
  num_workers: 2
  pin_memory: true # Usually good with GPU
  drop_last_batch: false
  # MODIFIED: Adjusted accumulation to target effective batch size of 64 (16 * 4 = 64)
  # Increase/decrease batch_size first, then adjust this.
  gradient_accumulation_steps: 4
  # --- Optimizer settings ---
  optimizer: "adamw"
  learning_rate: 1.0e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1.0e-8
  momentum: 0.9 # Only used if optimizer='sgd'
  # --- Scheduler settings ---
  scheduler: "cosine"
  warmup_epochs: 2
  warmup_lr_init: 1.0e-6
  min_lr: 1.0e-6
  # --- Gradient Clipping ---
  grad_clip_norm: 1.0
  # --- Mixed Precision ---
  use_amp: true # Keep true for T4 performance/memory

# === Validation Settings ===
validation:
  # MODIFIED: Validate less often to save time if validation is slow
  val_freq: 2
  # MODIFIED: Increased validation batch size (adjust if OOM)
  batch_size: 32
  best_metric: "val/loss_epoch"
  best_metric_mode: "min"

# === Logging Settings ===
logging:
  # MODIFIED: Log less frequently if training is faster
  log_freq: 200 # Log every N *optimizer steps*
  log_file: null
  level: "INFO"
  use_wandb: false # Keep false for simplicity unless you use WandB
  wandb:
    wandb_project: "cognitive_synergy"
    wandb_entity: null
    wandb_run_name: null

# === Checkpointing Settings ===
checkpointing:
  checkpoint_dir: "./checkpoints" # Saves to local Colab storage
  save_freq: 1 # Still relevant for 'save_last'
  save_best_only: true
  save_last: true
  resume_from_checkpoint: latest

# === Data Configuration ===
data:
  # Paths to processed manifest files (relative to project root)
  train_manifest: "./data/coco_train_manifest_processed.json"
  val_manifest: "./data/coco_val_manifest_processed.json"

  # CRITICAL FOR SPEED: Assumes you COPY data from Drive to local Colab storage first!
  # Example copy command: !cp -r /content/drive/MyDrive/COCO2017 /content/COCO2017_local
  image_root: /content/drive/MyDrive/COCO2017 # MODIFIED: Correctly points to the dataset subdir relative to project root
  # If using Drive directly (slower): image_root: /content/drive/MyDrive/COCO2017

  # Text processing settings
  max_text_length: 128
  # Image processing settings
  image_size: 224
